{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5698097-8ceb-4548-b205-0e390240eee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a88d704d-94ad-4955-ade8-8d8cb57f3c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "446a9e3c-579c-4cb7-9176-4d97e3240735",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_UNIQUE_CHARACTERS = 63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cdcce98-b19e-4a26-9242-5f7d1ad3df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('../data/X_train.npy')[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ec55359-ebd8-4d49-9fac-c00f9c56688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureExtraction = nn.Sequential(\n",
    "    nn.Conv1d(in_channels=164, out_channels=164, kernel_size=3, stride=2),\n",
    "    nn.BatchNorm1d(164),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    nn.Conv1d(in_channels=164, out_channels=164, kernel_size=3, stride=2),\n",
    "    nn.BatchNorm1d(164),\n",
    "    nn.LeakyReLU(0.1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddd49d2c-db55-426c-a376-e72db2f80cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 128, 164])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = torch.from_numpy(X_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a006fc6c-0955-47ad-bec3-cb374b9dd9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 128, 164])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_batches = X_train[:100]\n",
    "some_batches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31b6f25a-414a-4f51-b46f-a8568596f082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4088, 0.5199, 0.6122,\n",
       "        0.7076, 0.7973, 0.4947, 0.5328, 0.5536, 0.5662, 0.3912, 0.3466, 0.3294,\n",
       "        0.3103, 0.3035, 0.3326, 0.4078, 0.4528, 0.2413, 0.3030, 0.3730, 0.4131,\n",
       "        0.8284, 0.7978, 0.7294, 0.6939, 0.6774, 0.6424, 0.5563, 0.5016, 0.4530,\n",
       "        0.6481, 0.5527, 0.4906, 0.4391, 0.6801, 0.6451, 0.6971, 0.7392, 0.7284,\n",
       "        0.7304, 0.7702, 0.7986, 0.7106, 0.7122, 0.7119, 0.7126, 0.6911, 0.6725,\n",
       "        0.6600, 0.6449, 0.6512, 0.6709, 0.6825, 0.6965, 0.6930, 0.6958, 0.6703,\n",
       "        0.6618, 0.6615, 0.6522, 0.6818, 0.6758, 0.6508, 0.6617, 0.7308, 0.7523,\n",
       "        0.7673, 0.7870, 0.7806, 0.7582, 0.7446, 0.7292, 0.7333, 0.7291, 0.7582,\n",
       "        0.7676, 0.7689, 0.7787, 0.7446, 0.7520, 0.7790, 0.7692, 0.5274, 0.5378,\n",
       "        0.5387, 0.5521, 0.5253, 0.5296, 0.5342, 0.5416, 0.5409, 0.5390, 0.5380,\n",
       "        0.5376, 0.5513, 0.5384, 0.5391, 0.5464, 0.5398, 0.5440, 0.5385, 0.5492,\n",
       "        0.5381, 0.5400, 0.5254, 0.5301, 0.5346, 0.5414, 0.5407, 0.5391, 0.5381,\n",
       "        0.5377, 0.5514, 0.5384, 0.5390, 0.5465, 0.5397, 0.5440, 0.5384, 0.5494,\n",
       "        0.5382, 0.5400])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_batches[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9705158-cc89-449d-b7e2-cb74023768d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 128, 82, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = some_batches.view(100, 128, 82, 2)\n",
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9058a02f-c837-4668-a17a-9d82daaceefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignRecognition(nn.Module):\n",
    "    def __init__(self, frames, kernel_size=2, stride=2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(frames, frames // 4, kernel_size=kernel_size, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm2d(frames // 4)\n",
    "        self.lin1 = nn.Linear(41, 128)\n",
    "        self.lin2 = nn.Linear(128, N_UNIQUE_CHARACTERS)\n",
    "        self.gelu = torch.nn.GELU()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) # 128 41 1\n",
    "        print(x.shape)\n",
    "        x = self.gelu(self.bn1(x))\n",
    "        x = x.squeeze(dim=-1)\n",
    "        x = self.lin1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e1e9a4fb-6474-4c6d-ab79-125680cf643e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b6b1122c-7277-40b2-a67d-f0f91d3dccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SignRecognition(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c0f02c7-aa16-4327-852b-499d8f9c8369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 32, 41, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 32, 63])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = model(q)\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73a19177-7d9f-4722-b334-c80263327738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w[0, 0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "36b0e76b-d749-45c6-8049-dab85c4cb0a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 83])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg = torch.randint(0, 63, size=[2, 84])\n",
    "trg[:, :-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cd9c07d-28ed-4111-acfd-e3f26bcd3da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(embed_size, embed_size)\n",
    "        self.keys = nn.Linear(embed_size, embed_size)\n",
    "        self.queries = nn.Linear(embed_size, embed_size)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        # Get number of training examples\n",
    "        N = query.shape[0]\n",
    "\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        values = self.values(values)  # (N, value_len, embed_size)\n",
    "        keys = self.keys(keys)  # (N, key_len, embed_size)\n",
    "        queries = self.queries(query)  # (N, query_len, embed_size)\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = queries.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        # Einsum does matrix mult. for query*keys for each training example\n",
    "        # with every other training example, don't be confused by einsum\n",
    "        # it's just how I like doing matrix multiplication & bmm\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        # queries shape: (N, query_len, heads, heads_dim),\n",
    "        # keys shape: (N, key_len, heads, heads_dim)\n",
    "        # energy: (N, heads, query_len, key_len)\n",
    "\n",
    "        # Mask padded indices so their weights become 0\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        # Normalize energy values similarly to seq2seq + attention\n",
    "        # so that they sum to 1. Also divide by scaling factor for\n",
    "        # better stability\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "        # attention shape: (N, heads, query_len, key_len)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "        # attention shape: (N, heads, query_len, key_len)\n",
    "        # values shape: (N, value_len, heads, heads_dim)\n",
    "        # out after matrix multiply: (N, query_len, heads, head_dim), then\n",
    "        # we reshape and flatten the last two dimensions.\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        # Linear layer doesn't modify the shape, final shape will be\n",
    "        # (N, query_len, embed_size)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "\n",
    "        # Add skip connection, run through normalization and finally dropout\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        device,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_length,\n",
    "    ):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    dropout=dropout,\n",
    "                    forward_expansion=forward_expansion,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        out = self.dropout(\n",
    "            (self.word_embedding(x) + self.position_embedding(positions))\n",
    "        )\n",
    "\n",
    "        # In the Encoder the query, key, value are all the same, it's in the\n",
    "        # decoder this will change. This might look a bit odd in this case.\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.attention = SelfAttention(embed_size, heads=heads)\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            embed_size, heads, dropout, forward_expansion\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key, src_mask, trg_mask):\n",
    "        attention = self.attention(x, x, x, trg_mask)\n",
    "        query = self.dropout(self.norm(attention + x))\n",
    "        out = self.transformer_block(value, key, query, src_mask)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        trg_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        device,\n",
    "        max_length,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        trg_pad_idx,\n",
    "        embed_size=512,\n",
    "        num_layers=6,\n",
    "        forward_expansion=4,\n",
    "        heads=8,\n",
    "        dropout=0,\n",
    "        device=\"cpu\",\n",
    "        max_length=100,\n",
    "    ):\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            src_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            device,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            trg_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            device,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # (N, 1, 1, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        )\n",
    "\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be376491-0da6-4db8-9625-e681065e7d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_pad_idx = 0\n",
    "trg_pad_idx = 0\n",
    "src_vocab_size = 63\n",
    "trg_vocab_size = 63\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbfd0425-d377-4fcd-b02f-d639bbc393fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (word_embedding): Embedding(63, 512)\n",
       "    (position_embedding): Embedding(100, 512)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): SelfAttention(\n",
       "          (values): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (keys): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (queries): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (word_embedding): Embedding(63, 512)\n",
       "    (position_embedding): Embedding(100, 512)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x DecoderBlock(\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (values): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (keys): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (queries): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (transformer_block): TransformerBlock(\n",
       "          (attention): SelfAttention(\n",
       "            (values): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (keys): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (queries): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=512, out_features=63, bias=True)\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12820794-b48f-4ac7-8900-20a01f5272c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
